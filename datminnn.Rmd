---
title: "datmin"
author: "nyoba"
date: "2025-06-20"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Regresi Linear
n=1000
e=rnorm(n,0,5)    
x=runif(n,0,10)   
y=2+6*x+e         

# Estimasi Parameter
X=cbind(1, x)    
b=solve(t(X)%*%X)%*%t(X)%*%y   

# Evaluasi Model
y_hat=X%*%b   
SST=sum((y - mean(y))^2)
# Statistik Lanjutan
SSR=sum((y_hat - mean(y))^2)   
SSE=sum((y - y_hat)^2)       
MSR=SSR/1   
MSE=SSE/(n-2)   
R2=SSR/SST    # R-squared

cov_b=solve(t(X)%*%X)*MSE
se_b=sqrt(diag(cov_b))
t_hit=b/se_b   # t-hitung
p_val=2*(1-pt(abs(t_hit),n-2))   # p-value
res=y - y_hat    # Residuals

# Output hasil estimasi dan statistik lainnya
output=list("Hasil Estimasi", hasil, R2=R2, MSE=MSE)
output

```

```{r}
#PARAMETRIK
set.seed(123)
n <- 10
x <- rnorm(n, mean=0, sd=1)
x_bar <- mean(x)
var_x <- var(x)
sd_x <- sd(x)
B <- 100
x_bar_boot <- vector()
for (i in 1:B) {
  x_boot <- sample(x,n,replace = TRUE)
  x_bar_boot[i] <- mean(x_boot)
}

hist(x_bar_boot)
qqnorm(x_bar_boot)

#NONPARAMETRIK
set.seed(123)
x <- sample(seq(1,3,by=0.001),20, replace = TRUE)
n <- length(x)
x_bar <- mean(x)
var_x <- var(x)
sd_x <- sd(x)
var_x_bar <- var_x/n
sd_x_bar <- sqrt(var_x_bar)
B <- 100
x_bar_boot <- vector()
for (i in 1:B) {
  x_boot <- sample(x,n,replace = TRUE)
  x_bar_boot[i] <- mean(x_boot)
}

hist(x_bar_boot)
qqnorm(x_bar_boot)
```


```{r}
#jackknife
set.seed(123)
x = rnorm(1000, mean=160, sd=4)
n = length(x)
partials = rep(0,n)
for (i in 1:n){
  partials[i] = mean(x[-i])
}

partials
pseudos = (n*mean(x)) - (n-1)*partials

par(mfrow=c(2:1), pin=c(5,1)) 
hist(partials)
#jackknife estimation
jack.est = mean(pseudos)
#jackknife standard error
jack.se = sqrt(var(pseudos)/n)
CI = qt(0.05/2,n-1, lower.tail = FALSE)*jack.se
lower = jack.est - CI
upper = jack.est + CI
lower
upper
```

```{r}

beta <- solve(t(X) %*% X) %*% t(X) %*% y
res <- y - X %*% beta
res_std <- (res - mean(res)) / sd(res)
abs_res <- abs(res)

## use R build-in OLS estimaor (lm())
reg <- lm(weight ~ height, data=df)
summary(reg)

## use our own function
OLS(y=df$weight,X=df$height)

ks_result <- ks.test(res_std, "pnorm", mean = 0, sd = 1)
shapiro_result <- shapiro.test(res)
glejser_model <- lm(abs_res ~ X[,2])

#anova
ANOVA1 <- aov(y1 ~ perlakuan, data = Data1)
ANOVA2 <- aov(y2 ~ Perlakuan_A + Perlakuan_B, data = Data2)
INTERACTION <- aov(y2 ~ Perlakuan_A * Perlakuan_B, data = Data2)
ANOVA3 <- aov(y3 ~ Treatments + Block, data = Data3)
ANOVA4 <- aov(y1 ~ Perlakuan+Baris+Kolom, data = Data4)
model1 <- aov(yield ~ tekanan + batch, data = datasoal3)
ANOVA2 <- aov(y2 ~ jb+temp+jb*temp, data = Data5)
summary(model1)

```

```{r}
# Distribusi Bernouli
set.seed(123)
library(Rlab)
x <- rbern(100, 0.5)
bernoulli_lik <- function(x, p) {
  prod(p^x * (1 - p)^(1 - x))
}
# negative log-likelihood function
bernoulli_nllik <- function(x, p) {
  -sum(log(bernoulli_lik(x, p)))
}
# estimate parameter
res_bernoulli <- optimize(f = bernoulli_nllik, interval = c(0, 1), x = x)

# print result
print(paste0("MLE estimate for Bernoulli distribution: ", res_bernoulli$minimum))

#Distribusi Binomial
# generate data
set.seed(123)
x <- rbinom(100, 10, 0.5)

# likelihood function
binomial_lik <- function(x, n, p) {
  prod(choose(n, x) * p^x * (1 - p)^(n - x))
}

# negative log-likelihood function
binomial_nllik <- function(x, n, p) {
  -sum(log(binomial_lik(x, n, p)))
}

# estimate parameter
res_binomial <- optim(par = c(0.3), binomial_nllik, x = x, n = 10)

# print result
print(paste0("MLE estimate for Binomial distribution: ", res_binomial$par))

llik = function(x,par){
  n = length(x)
  a = par[1]
  b= par[2]
  z = sum(x)
  ll =  (n*(-a*log(b)-log(gamma(a))))+(a - 1)* sum(log(x))-(1/b)*sum(x)
  return(-ll)
}

resgamma = optim(par=c(0.5,0.5),llik,x=x)
resgamma$par

```

```{r}
ibrary(BSDA)

#uji Z 1 populasi #jika jumlah n > 30
#h0: miu <= miu0
#h1: miu > miu0
#right tail
zsum.test(mean.x = 1200000, sigma.x = 600000, n.x = 16,
          alternative = "greater", mu = 0,
          conf.level = 0.95) #H0: <=, H1: > (greater)

#uji t 1 populasi #jika jumlah sampel n < 30
#H0: miu = miu0
#H1: miu =/ miu0
tsum.test(mean.x = 25, s.x = 6.4, n.x = 20, 
          alternative = "two.sided", mu = 0,
          var.equal = TRUE, conf.level = 0.95)

#uji 2 populasi independen/tak berpasangan
#H0: miu1-miu2 = 0
#H1: miu1-miu2 =/ 0
t.test(x = IR64, y = MSP, alternative = "two.sided",
       paired = FALSE, var.equal = TRUE,
       conf.level = 0.95)

#uji t berpasangan
Orang.ke=seq(1:10)
BB.Sebelum=c(57,69,56,67,55,56,62,67,67,56)
BB.Sesudah=c(55,70,56,65,54,55,64,65,67,54)
data=data.frame(Orang.ke,BB.Sebelum,BB.Sesudah)

#H0: miu1-miu2 <= 0
#H1: miu1-miu2 > 0
t.test(x = data$BB.Sebelum, y = data$BB.Sesudah, alternative = "greater",
       mu = 0.5, paired = TRUE, var.equal = TRUE,
       conf.level = 0.95)

```

```{r}
# === REGRESI LINIER SEDERHANA (UAS KOMPUTASI STATISTIKA) ===

# 1. Simulasi Data
n <- 1000
e <- rnorm(n, 0, 0.5)                   # error ~ N(0, 0.5^2)
x <- runif(n, 0, 10)                   # x ~ Uniform(0, 10)
y <- 2 + 6 * x + e                     # model: y = 2 + 6x + e

# 2. Estimasi Parameter Model
X <- cbind(1, x)
b <- solve(t(X) %*% X) %*% t(X) %*% y  # beta_hat

# 3. Evaluasi Model
y_hat <- X %*% b
SST <- sum((y - mean(y))^2)
SSR <- sum((y_hat - mean(y))^2)
SSE <- sum((y - y_hat)^2)
MSR <- SSR / 1
MSE <- SSE / (n - 2)
R2 <- SSR / SST

# 4. Inferensi Koefisien
cov_b <- solve(t(X) %*% X) * MSE
se_b <- sqrt(diag(cov_b))
t_hit <- b / se_b
p_val <- 2 * (1 - pt(abs(t_hit), n - 2))
res <- y - y_hat
hasil <- cbind(b, se_b, t_hit, p_val)
dimnames(hasil) <- list(c("Intercept", "x"), c("Estimate", "SE", "t", "p"))

# 5. Kurva Regresi & Interval Kepercayaan
s_y_hat <- sqrt(MSE * diag(X %*% solve(t(X) %*% X) %*% t(X)))
t_tab <- abs(qt(0.025, n - 2))
ba_y_hat <- y_hat - t_tab * s_y_hat
bb_y_hat <- y_hat + t_tab * s_y_hat
garis <- c("Kurva regresi", "Batas Interval")

windows()
plot(x, y, xlab = "x", ylab = "y", main = "Kurva Regresi Linier")
lines(x[order(x)], y_hat[order(x)])
lines(x[order(x)], ba_y_hat[order(x)], col = "red", lty = 2)
lines(x[order(x)], bb_y_hat[order(x)], col = "red", lty = 2)
legend(min(x), max(y), garis, lty = c(1, 2), col = c(1, 2))

# 6. Pemeriksaan Residual
windows()
par(mfrow = c(2, 2))
hist(res, main = "Histogram Residual", col = "lightblue")
qqnorm(res); qqline(res)
plot(res, y_hat, main = "Plot Residual vs y_hat")
plot.ts(res, main = "Time Series Residual")

# 7. Output Final
output <- list("Hasil Estimasi" = hasil, R2 = R2, MSE = MSE)
output

```

```{r}
# Set seed untuk replikasi
set.seed(123)

# Parameter populasi
n <- 30
lambda <- 1 / 25  # karena mean = 25 → λ = 1/mean

# 1. Generate sampel acak dari distribusi eksponensial
sampel <- rexp(n, rate = lambda)

# --- a) Bootstrap Percentile Method ---

# Jumlah resample
n_boot <- 10000

# Simpan rata-rata dari tiap resample
boot_means <- numeric(n_boot)
for (i in 1:n_boot) {
  resample_i <- sample(sampel, size = n, replace = TRUE)
  boot_means[i] <- mean(resample_i)
}

# Hitung interval kepercayaan bootstrap percentil 95%
ci_percentile <- quantile(boot_means, probs = c(0.025, 0.975))

# --- b) Bootstrap-t Method ---

# Langkah ii & iii
x_bar <- mean(sampel)
s <- sd(sampel)

# Langkah v & vi
boot_sds <- numeric(n_boot)
boot_means_b <- numeric(n_boot)
T_stats <- numeric(n_boot)

for (i in 1:n_boot) {
  resample_i <- sample(sampel, size = n, replace = TRUE)
  boot_mean <- mean(resample_i)
  boot_sd <- sd(resample_i)
  
  boot_means_b[i] <- boot_mean
  boot_sds[i] <- boot_sd
  T_stats[i] <- (boot_mean - x_bar) / (boot_sd / sqrt(n))  # Rumus (1)
}

# Langkah viii: hitung quantile dari T statistic
t_q <- quantile(T_stats, probs = c(0.975, 0.025))  # diperhatikan urutan

# Rumus (2): Interval Kepercayaan berdasarkan distribusi T bootstrap
ci_t <- c(
  x_bar - t_q[1] * (s / sqrt(n)),
  x_bar - t_q[2] * (s / sqrt(n))
)

# --- Output Hasil ---

cat("Rata-rata sampel:\n", x_bar, "\n\n")
cat("Interval Percentile Bootstrap (95%):\n", ci_percentile, "\n\n")
cat("Interval Bootstrap-t (95%):\n", ci_t, "\n\n")

```

```{r}
X = c(0.388495237, 0.495981747, 1.828324201, 0.009331076,
      2.168366877, 0.293000151, 0.223063166, 3.778375571,
      1.374471450, 0.415257032)
# Fungsi negatif log-likelihood untuk distribusi eksponensial
neg_log_likelihood <- function(lambda, data) {
  if (lambda <= 0) return(Inf)  # lambda harus positif
  n <- length(data)
  - (n * log(lambda) - lambda * sum(data))  # negatif dari log-likelihood
}
# Optimisasi menggunakan fungsi buatan
result <- optim(par = 1,                          # initial value
                fn = neg_log_likelihood,         # fungsi yang diminimalkan
                data = data_X,
                method = "Brent",                # metode numerik 1D
                lower = 0.0001, upper = 100)     # batas domain

# Tampilkan hasil estimasi lambda
cat("Estimasi MLE untuk lambda adalah:", result$par, "\n")
```

```{r}
# Data
kerut_perokok <- 95
total_perokok <- 150

kerut_nonperokok <- 103
total_nonperokok <- 250

# Uji proporsi dua sampel (alternative = "greater")
prop.test(x = c(kerut_perokok, kerut_nonperokok),
          n = c(total_perokok, total_nonperokok),
          alternative = "greater",
          correct = FALSE)  # tanpa continuity correction

```

