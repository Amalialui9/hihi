---
title: "Komputasi Statistika Pertemuan 11"
author: "Amalia Bakti Pertiwi"
date: "2025-05-16"
output:
  pdf_document: default
  html_document: default
---

Maximum Likelihood Estimation and optimatization in R

```{r}
library(stats4)
library(MASS)
xpoisson=rpois(n=300, lambda=2)
```

```{r}
lpoisson=function(mu)
{
n=length(xpoisson)
x=xpoisson
for (i in 1:n){
lnlikeli=(n*mu-sum(x)*log(mu)+sum(log(factorial(x))))
}
return(lnlikeli)
}
estpoisson=mle(minuslogl=lpoisson, start=list(mu=2))
summary(estpoisson)
```

```{r}
lpoisson = function(mu) {
  x = xpoisson
  n = length(x)
  ll = n*mu-sum(x)*log(mu)+sum(log(factorial(x)))
  return(-ll)
}
estpoisson = mle(minuslogl = lpoisson, start = list(mu = 2), method = "Brent", lower = 0.0001, upper = 10)
summary(estpoisson)
```

## Univariate Example

## Distribusi Normal

```{r}
#Estimasi parameter distribusi normal
set.seed(123) # for consistency set the seed explicitly.
#first simulate some normal data with expected mean of 0 and sd of 1
x = rnorm(100)
# scale the data the way that we would like
x = x/sd(x) * 8 # sd of 8
x = x-mean(x) + 10 # mean of 10
c('mean'=mean(x),'sd'=sd(x)) # double check
```

```{r}
# histogram (in the fashion of SPSS)
hist(x, freq=FALSE,col='tan')
lines(density(x),col='red',lwd=2)

```

```{r}
#Since these data are drawn from a Normal distribution,
#we will use the Gaussian Normal distribution function for fitting

# specify the single value normal probability function
norm_lik = function(x, m, s){
  y = 1/sqrt(2*pi*s^2)*exp((-1/(2*s^2))*(x-m)^2)
}
# and plot it just to make sure
plot(seq(-3,3,.1),sapply(seq(-3,3,.1),FUN=norm_lik,m=0,s=1),type='l',
     ylab='',xlab='', main='Gaussian Normal')
```

```{r}
#create a likelihood function for normal distribution
llik = function(x,par){
    m=par[1]
    s=par[2]
    n=length(x)
    # log of the normal likelihood
    # -n/2 * log(2*pi*s^2) + (-1/(2*s^2)) * sum((x-m)^2)
    ll = -(n/2)*(log(2*pi*s^2)) + (-1/(2*s^2)) * sum((x-m)^2)
    # return the negative to maximize rather than minimize
    return(-ll)
  }

# log likelihood curve
plot(seq(-3,3,.1),-1*sapply(seq(-3,3,.1),FUN=llik,par=c(0,1)),type='l',
     ylab='',xlab='')  
```

```{r}
# negative log likelihood curve
# just to see what the funciton produces we can plot it.
plot(seq(-3,3,.1),sapply(seq(-3,3,.1),FUN=llik,par=c(0,1)),type='l',
     ylab='',xlab='')
```

```{r}
# call optim with the starting values 'par',
# the function (here 'llik'),
# and the observations 'x'
res0 = optim(par=c(.5,.5), llik, x=x)

library(knitr)
print(kable(
  cbind('direct'=c('mean'=mean(x),'sd'=sd(x)),
        'optim'=res0$par),digits=3))
```

## Bivariate Example (Regression)

```{r}
#Estimasi parameter regresi
# our totally made up data
MomEd = c(0, 1, 3, 4) #mom's education
CGPA = c(3.0, 3.2, 3.3, 3.7) #children's GPA
# fit a linear model
coef(lm(CGPA ~ MomEd)->lm0)


```

Using numerical optimization

Metode lain untuk mendapatkan estimasi parameter yaitu melibatkan definisi dari loss function dan meminimalkannya dengan opmitization. Fungsi kerugian yang kita pilih adalah jumlah kuadrat terkecil yang membandingkan prediksi kita dengan nilai yang diamati.

```{r}
# sum of squares function
SS_min = function(data,par){
  b0=par[1]
  b1=par[2]
  loss = with(data, sum((b0+b1*x - y)^2))
  return(loss)
}
# data on mom's ed and Children GPA from above
dat=data.frame(x=MomEd,y=CGPA)
# min resid sum of squares
res1a = optim(par=c(.5,.5),SS_min, data=dat)
# direct comparison
print(kable(cbind('lm()'=coef(lm0),'SS_min'=res1a$par),digits=4))
```

Pertama, kita melakukan simulasi data bivariate

```{r}
# slope effect
b1 = .85
# simulated data
x = 1:60
dat = data.frame(x=x,y=(x*b1)+rnorm(60))
# from the lm() function
lm1 = lm(y~x, data=dat)
lm1
```

sekarang kita compare denga fungsi optim()

```{r}
# different start values
res1 = optim(par=c(.01,1),SS_min, data=dat)
res1$par
```

```{r}
op=par(mfrow=c(1,2),mar=c(3,3,3,1),pty='s')
# scatterplot
with(dat,plot(x,y,col='grey60',main='lm() results'))
# regression line from lm()
abline(lm1,lwd=2)
with(dat,plot(x,y,col='grey60',main='optim() results'))
# from the Min SS
abline(res1$par[1],res1$par[2],col='red',lty=1,lwd=2)
```

```{r}
#Latihan soal
```

```{r}
library(stats4)
library(MASS)
```

```{r}
set.seed(123)
n <- 10 
m <- 100
p_true <- 0.5
k <- rbinom(m, size = n, prob = p_true)

#Pertama kita mengambil sample terlebih dahulu menggunakan rbinom dengan n=10, m=100 dan pwluang sukseknya = 0.5

llik_binom <- function(par) {
  p <- par[1]
  size <- n 
  ll <- sum(lchoose(size, k)) + (sum(k) * log(p)) + (sum(size - k) * log(1 - p)) #ini adalah fungsi log likelihood dari pmf distribusi binomial
  return(-ll)
}

#Lalu kita mengiterasikan untuk menghitung nilai negatif parameter dari distribusi binomial yang memiliki satu parameter yaitu p yang berdasarkan sample acak k dan perulangan binomial sebanyak n

res_binom <- optim(par = c(0.5), fn = llik_binom, method = "Brent", lower = 0.0001, upper = 0.9999)

#Di sini kita mencari nilai estimasi parameter p untuk distribusi binomial menggunakan metode MLE dengan fungsi optim() yang digunakan untuk mencari nilai minimum dari fungsi netaif log (llik_binom). 

cat("Estimasi MLE untuk p =", res_binom$par, "\n")

```
Hasil estimasi unutk distribusi binomial dengan MLE adalah 0.499. Hal ini menunjukkan hasil estimasi mendekati nilai paramater awal p yaitu 0.5
```{r}
set.seed(123)
n <- 100
alpha <- 2
lambda <- 1
x <- rgamma(n, shape = alpha, rate = lambda)

#Pertama kita ambil sampel acak x  dari distrubusi gamma dengan jumlah sample 100, alpha 2 dan lambda 1. 

llik_gamma <- function(par) {
  alpha <- par[1]
  lambda <- par[2]
  # Fungsi ini menerima dua parameter yaitu alpha dan lambda. 
  
  if (alpha <= 0 || lambda <= 0) return(Inf)
  n <- length(x)
  ll <- n * alpha * log(lambda) - n * lgamma(alpha) +
        (alpha - 1) * sum(log(x)) - lambda * sum(x)
# ini adalah fungsi log likelihoodnya
# Log(α,λ)= ∑[αlog(λ)−logΓ(α)+(α−1)log(xi)−λxi]
# logL(α,λ∣x)=nαlog(λ)−nlogΓ(α)+(α−1)∑logxi−λ∑xi

  return(-ll)
}
#Di sini kita membuat fungsi loglikelihood negatif dari distribusi gamma. 

res_gamma <- optim(par = c(1, 1), fn = llik_gamma, method = "L-BFGS-B",
                   lower = c(0.0001, 0.0001), upper = c(Inf, Inf))
# ini merupakan fungsi digunakan untuk meminimalkan fungsi neatif likelihood sehingga hsail yang didapatkan adalah maksismal

cat("Estimasi MLE untuk alpha =", res_gamma$par[1], "\n")
cat("Estimasi MLE untuk lambda =", res_gamma$par[2], "\n")


```
Hasil estimsai dari parameter alpha sebesar 2,193214 yang jika dibulatkan adalah 2 diman anilai ini sama degan nilai parameter alpha yang kita set sebesar 2.
Hasil estimasi untuk lambda sebesar 1.274013 yang jika dibulatkan bernilai 1 dimananilai ini sama degan parameter lambda yang kita set di aal sebesar 1.

```{r}
set.seed(123)

xm <- 1               
alpha_true <- 3       
n <- 95
x <- xm / runif(n)^(1 / alpha_true)
#Pertama kita mengambil sampel acak dari distribusi pareto dimana kita set xm(nilai minimum) sebesar satu dan kita set parameter alfa bernilai 3. 

llik_pareto <- function(par) {
  alpha <- par[1]
  
  if (alpha <= 0) return(Inf)
  if (any(x < xm)) return(Inf)
  
  n <- length(x)
  ll <- n * log(alpha) + n * alpha * log(xm) - (alpha + 1) * sum(log(x))
  # log likehoodnya 
  #∑[log(α)+αlog(xm)−(α+1)log(xi)]
  #nlog(α)+nαlog(xm)−(α+1)∑log(xi)
  
  return(-ll)  
}
#Lalu kita membuat fungsi negatif log likelihood distribusi pareto dengan parameter alfa. disini kita beri syarat jika alpha bernilai negatif maka akan dikembalikan inf karena alpha harus bernilai positif. Dan unutk nilai x yang kurang adri nilai xm maka akan dikembalikan inf karena distribusi pareto hanya menerima x yang bernilai lebih besar dari nilai xm

res_pareto <- optim(par = c(1), fn = llik_pareto, method = "Brent",
                    lower = 0.0001, upper = 100)

#Disni ita melakukan estimasi fungsi negatif likelihood dengan menggunakan metode MLE untuk mendapatkan nilai maximum

cat("Estimasi MLE untuk alpha =", res_pareto$par, "\n")

```
Hasil dari estimasi MLE distribusi pareto didapatkan nilai alpha 3.0524 yang jika dibulatkan bernilai 3. Nilai ini sama dengan nilai parameter alpha yang kita set di aal sebesar alpha = 3

#Univariate
set.seed(123) # for consistency set the seed explicitly.
#first simulate some normal data with expected mean of 0 and sd of 1
x = rnorm(100)
# scale the data the way that we would like
x = x/sd(x) * 8 # sd of 8
x = x-mean(x) + 10 # mean of 10
c('mean'=mean(x),'sd'=sd(x)) # double check


# histogram (in the fashion of SPSS)
hist(x, freq=FALSE,col='tan')
lines(density(x),col='red',lwd=2)

#Since these data are drawn from a Normal distribution,
#we will use the Gaussian Normal distribution function for fitting

# specify the single value normal probability function
norm_lik = function(x, m, s){
  y = 1/sqrt(2*pi*s^2)*exp((-1/(2*s^2))*(x-m)^2)
}

# and plot it just to make sure
# seq(-3,3,.1) Membuat urutan angka dari -3 ke 3 dengan langkah 0.1.
plot(seq(-3,3,.1),
     sapply(seq(-3,3,.1),
            FUN=norm_lik,
            m=0,
            s=1),
     type='l',
     ylab='',
     xlab='', 
     main='Gaussian Normal')

# type l artinya lineplot

#create a likelihood function for normal distribution
# par bentuknya vector numeric
# optim() pada dasarnya mencari nilai minimum dari fungsi yang diberikan.
llik = function(x,par){
  m=par[1]
  s=par[2]
  n=length(x)
  # log of the normal likelihood
  # -n/2 * log(2*pi*s^2) + (-1/(2*s^2)) * sum((x-m)^2)
  ll = -(n/2)*(log(2*pi*s^2)) + (-1/(2*s^2)) * sum((x-m)^2)
  # return the negative to maximize rather than minimize
  return(-ll)
}

# log likelihood curve
plot(seq(-3,3,.1),-1*sapply(seq(-3,3,.1),FUN=llik,par=c(0,1)),type='l',
     ylab='',xlab='')  

# negative log likelihood curve
# just to see what the funciton produces we can plot it.
plot(seq(-3,3,.1),sapply(seq(-3,3,.1),FUN=llik,par=c(0,1)),type='l',
     ylab='',xlab='')

# call optim with the starting values 'par',
# the function (here 'llik'),
# and the observations 'x'
res0 = optim(par=c(.5,.5), llik, x=x) 
#par c itu parameter yang diinginkan (0.5 itu mengira ngira aja)

library(knitr)
print(kable(
  cbind('direct'=c('mean'=mean(x),'sd'=sd(x)),
        'optim'=res0$par),digits=3))



#Bivariate example (regression) #least square (cari error terus dikuadratkan)
# our totally made up data
MomEd = c(0, 1, 3, 4) #mom's education
CGPA = c(3.0, 3.2, 3.3, 3.7) #children's GPA
# fit a linear model
coef(lm(CGPA ~ MomEd)->lm0)

# sum of squares function
SS_min = function(data,par){
  b0=par[1]
  b1=par[2]
  loss = with(data, sum((b0+b1*x - y)^2)) #ini least square
  return(loss)
}

# data on mom's ed and Children GPA from above
dat=data.frame(x=MomEd,y=CGPA)

# min resid sum of squares
res1a = optim(par=c(.5,.5),SS_min, data=dat) 
#par c itu parameter yang diinginkan (0.5 itu mengira ngira aja) kalo ga diketahui gapapa ngira aja 0.5. Untuk initial value bebas

# direct comparison
print(kable(cbind('lm()'=coef(lm0),'SS_min'=res1a$par),digits=4)) 
#untuk beta 1 gaada bias karena gaada selisih, kalo yang beta0 ada bias karena ada selisihnya

# slope effect
b1 = .85
# simulated data
x = 1:60
dat = data.frame(x=x,y=(x*b1)+rnorm(60))
# from the lm() function
lm1 = lm(y~x, data=dat)
lm1

# different start values
res1 = optim(par=c(.01,1),SS_min, data=dat)
res1$par

op=par(mfrow=c(1,2),mar=c(3,3,3,1),pty='s')
# scatterplot
with(dat,plot(x,y,col='grey60',main='lm() results'))
# regression line from lm()
abline(lm1,lwd=2)
with(dat,plot(x,y,col='grey60',main='optim() results'))
# from the Min SS
abline(res1$par[1],res1$par[2],col='red',lty=1,lwd=2)


#estimasi parameter distribusi poisson
set.seed(123)
#first simulate some normal data with expected mean of 0 and sd of 1
x2 = rpois(100, 20)

# histogram (in the fashion of SPSS)
hist(x2, freq=FALSE,col='tan')
lines(density(x2),col='red',lwd=2)

pmf_pois = function(x, lambda) {
  y = exp(lambda) * 1 / factorial(x) * lambda ^ x
  return(y)
}

plot(seq(0, 20, 1), 
     sapply(seq(0, 20, 1), 
            FUN = pmf_pois,
            lambda = 4),
     type='l',
     main='PMF Poisson lambda 4')


llik = function(x, par) {
  lambda = par[1]
  n = length(x)
  ll = -n * lambda - sum(log(factorial(x))) + log(lambda) * sum(x)
  return(-ll)
}

res0 = optim(par=c(0.5), llik,x=x2)
res0$par

library(knitr)
print(kable(
  cbind(
    'direct'=c('mean'=mean(x2)),
    'optim'=res0$par
  ), digits=3))

# Distribusi Bernouli
# generate data
set.seed(123)
library(Rlab)
x <- rbern(100, 0.5)

# likelihood function
bernoulli_lik <- function(x, p) {
  prod(p^x * (1 - p)^(1 - x))
}

# negative log-likelihood function
bernoulli_nllik <- function(x, p) {
  -sum(log(bernoulli_lik(x, p)))
}

# estimate parameter
res_bernoulli <- optimize(f = bernoulli_nllik, interval = c(0, 1), x = x)

# print result
print(paste0("MLE estimate for Bernoulli distribution: ", res_bernoulli$minimum))



#Distribusi Binomial
# generate data
set.seed(123)
x <- rbinom(100, 10, 0.5)

# likelihood function
binomial_lik <- function(x, n, p) {
  prod(choose(n, x) * p^x * (1 - p)^(n - x))
}

# negative log-likelihood function
binomial_nllik <- function(x, n, p) {
  -sum(log(binomial_lik(x, n, p)))
}

# estimate parameter
res_binomial <- optim(par = c(0.3), binomial_nllik, x = x, n = 10)

# print result
print(paste0("MLE estimate for Binomial distribution: ", res_binomial$par))


#Distribusi Exponensial
# generate data
set.seed(123)
x <- rexp(120, 5)

# likelihood function
exponential_lik <- function(x, lambda) {
  prod(lambda * exp(-lambda * x))
}

# negative log-likelihood function
exponential_nllik <- function(x, lambda) {
  -sum(log(exponential_lik(x, lambda)))
}

# estimate parameter
res_exponential <- optim(par = 0.5, exponential_nllik, x = x)

# print result
print(paste0("MLE estimate for Exponential distribution: ", res_exponential$par))


# Distribusi Gamma
# generate data
set.seed(123)
x <- rgamma(50, 1, 2)

# likelihood function
gamma_lik <- function(x, shape, rate) {
  prod(dgamma(x, shape, rate))
}

# negative log-likelihood function
gamma_nllik <- function(x, shape, rate) {
  -sum(log(gamma_lik(x, shape, rate)))
}

# estimate parameter
res_gamma <- optim(par = c(0.5, 0.5), gamma_nllik, x = x, shape = 5)

# print result
print(paste0("MLE estimate for Gamma distribution: ", res_gamma$par))

#pareto
set.seed(123)
library(Pareto)
x <- rPareto(50, 2, 1)

pmfpareto <-function(x, t, alpha){
  pem = alpha * t^alpha
  pen = x^(alpha+1)
  return(pem/pen)
}

#likelihood
logpareto <- function(x,par){
  t <- par[1]
  alpha <- par[2]
  logpare = sum(log(pmfpareto(x,t,alpha)))
  return(-logpare)
}

respar <- optim(par = c(5, 5), logpareto, x?=?x)
respar$par


#Gamma
set.seed(123)
x=rgamma(50,2,1)

#gamma_lik = function(x,a,b){
#  y = ((b^a)/(gamma(a)))*x^(a-1)*exp(-b*x)
#  return(y)
#}

llik = function(x,par){
  n = length(x)
  a = par[1]
  b= par[2]
  z = sum(x)
  ll =  (n*(-a*log(b)-log(gamma(a))))+(a - 1)* sum(log(x))-(1/b)*sum(x)
  return(-ll)
}

resgamma = optim(par=c(0.5,0.5),llik,x=x)
resgamma$par

